{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RbelOPK4TFB"
   },
   "source": [
    "# business and data understanding\n",
    "------------\n",
    "\n",
    "The initial phase is concerned with tasks to define the business objectives and translate it to ML objectives, to collect and verify the data quality and to finaly assess the project feasibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/55J7fBc.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "\n",
    "### tasks\n",
    "\n",
    "Compile a glossary of terminology relevant to the project. This may include two components: \n",
    "(1) A glossary of relevant business terminology, which forms part of the business understanding available to the project. Constructing this glossary is a useful \"knowledge elicitation\" and education exercise.\n",
    "(2) A glossary of machine learning terminology, illustrated with examples relevant to the business problem in question.\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. Business terminology\n",
    "A table or paragraph contains all the business related terms to be used in the project.\n",
    "\n",
    "> ##### Example: \n",
    "> - Key Performance Indicators (KPIs): Quantifiable metrics used to measure the performance of a business or organization. The company's KPIs include revenue growth, customer satisfaction, and employee retention.\n",
    "> - Market Segmentation: Dividing a market into distinct groups based on demographics, needs, or preferences. Example: \"The company segmented its target market into young professionals, families, and retirees to tailor marketing strategies.\"\n",
    "> Return on Investment (ROI): The return or profit generated by an investment compared to its cost. Example: \"The company calculated a 20% ROI on its new marketing campaign, indicating a successful investment.\"\n",
    "\n",
    "#### 2. ML terminology\n",
    "A table or paragraph contains all the ML related terms to be used in the project.\n",
    "\n",
    "\n",
    "> ##### Example: \n",
    "> Underfitting: When a machine learning model is too simple and fails to capture the underlying patterns in the data. Example: \"The company's initial model was underfitting the data, so they added more features and layers to improve accuracy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## <span style=\"color:green\">**Key Performance Indicators (KPIs)**</span>\n",
    "\n",
    "- <span style=\"color:green\">Streams</span>: Number of times the track has been listened to. Higher stream counts generally indicate higher popularity.\n",
    "\n",
    "- <span style=\"color:green\">Artist Followers</span>: Number of followers of the track's author. More followers can correlate with higher track popularity.\n",
    "\n",
    "- <span style=\"color:green\">Artist Popularity</span>: Popularity of the track's author estimated by Spotify. This metric can influence the track's exposure and, consequently, its popularity.\n",
    "\n",
    "- <span style=\"color:green\">Trend</span>: Change in track's position within the chart, indicating the trajectory of the track's popularity.\n",
    "\n",
    "- <span style=\"color:green\">Popularity</span>: Track's popularity score on Spotify. This is main metric we try to predict.\n",
    "\n",
    "\n",
    "## <span style=\"color:green\">**Market Segmentation**</span>\n",
    "\n",
    "- <span style=\"color:green\">Genres</span>: Track genres. Different genres appeal to different listener demographics and can be used to segment the market.\n",
    "\n",
    "- <span style=\"color:green\">Region</span>: Region of the chart, if the track is part of any chart. This helps in understanding regional preferences and segmenting the market geographically.\n",
    "\n",
    "- <span style=\"color:green\">Available Markets</span>: Countries where the track is available. This information helps segment the market by geographical availability.\n",
    "\n",
    "- <span style=\"color:green\">Album Release Date</span>: The date the album was released. This can help segment the market based on temporal factors, such as the time of year or concurrent releases.\n",
    "\n",
    "- <span style=\"color:green\">Track Artists</span>: Name of the track's author. Collaborations and artist reputation can help segment the market by fan bases.\n",
    "\n",
    "- <span style=\"color:green\">Explicit</span>: Whether the lyrics contain obscene words. This can segment the market by age group or content preference.\n",
    "\n",
    "## <span style=\"color:green\">**ML Terminology**</span>\n",
    "\n",
    "- <span style=\"color:green\">Supervised Learning</span>: A machine learning approach where the model learns from labeled training data to make predictions or classifications. We train the model to predict the song popularity on a labeled data with features and popularity score.\n",
    "\n",
    "- <span style=\"color:green\">Regression</span>: A type of supervised learning task where the model predicts a continuous value or quantity. Song popularity is a continous value so we build a regression model.\n",
    "\n",
    "- <span style=\"color:green\">Model Evaluation</span>: Assessing the performance of a machine learning model using various metrics such as mean squared or mean absolute error.\n",
    "\n",
    "- <span style=\"color:green\">Overfitting</span>: When a machine learning model performs well on the training data but fails to generalize to new, unseen data. This can happen due to change of trends with a time.\n",
    "\n",
    "- <span style=\"color:green\">Hyperparameter Tuning</span>: The process of selecting the optimal values for the hyperparameters of a machine learning model to improve its performance. For example depth and number of decision trees for ensemble learning.\n",
    "\n",
    "- <span style=\"color:green\">Cross-Validation</span>: A technique used to assess the performance of a machine learning model by splitting the data into multiple subsets for training and testing.\n",
    "\n",
    "- <span style=\"color:green\">Ensemble Learning</span>: A technique where multiple machine learning models are combined to make predictions, often resulting in improved performance.\n",
    "\n",
    "- <span style=\"color:green\">Natural Language Processing (NLP)</span>: The field of study that focuses on the interaction between computers and human language, enabling machines to understand, interpret, and generate human language. In our data we have a plain text which we can use for better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wfrdzmhe0Zfr"
   },
   "source": [
    "## Scope of the project\n",
    "----------\n",
    "\n",
    "### tasks\n",
    "- Explore the background of the business.\n",
    "- Define business problem\n",
    "- Define business objectives\n",
    "- Translate business objectives into ML objectives\n",
    "\n",
    "The objective here is to thoroughly understand, from a business perspective, what the client really wants to accomplish. Often the client has many competing objectives and constraints that must be properly balanced. The goal is to uncover important factors, at the beginning, that can influence the outcome of the project. A possible consequence of neglecting this step is to expend a great deal of effort producing the right answers to the wrong questions.\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. background\n",
    "\n",
    "A short paragraph to record the information that is known about the organization's business situation at the beginning of the project.\n",
    "\n",
    "> ##### Example:\n",
    "> Shopzilla is an an e-commerce platform. They have branches in different cities. They have a Mobile app which allows clients to buy products. They profile the purchase history of the users and use this data for building AI models.\n",
    "\n",
    "#### 2. business problem\n",
    "A short paragraph to describe the business problem.\n",
    "\n",
    "> ##### Example:\n",
    "> The business problem is that the business stakeholders wants to follow a customer centric sales methodology. They want to predict customer satisfaction in their services such that they can adapt their strategies accordingly. The provided dataset is labelled and captures customer satisfaction scores for a one-month period. It includes various features such as category and sub-category of interaction, customer remarks, survey response date, category, item price, agent details (name, supervisor, manager), and CSAT score etc.\n",
    "> The company has been experiencing high customer churn rates, resulting in significant revenue losses.\n",
    "\n",
    "#### 3. business objectives \n",
    "\n",
    "A list of business objectives which describes the customer's primary objective, from a business perspective. In addition to the primary business objective, there are typically other related business questions that the customer would like to address. For example, the primary business goal might be to keep current customers by predicting when they are prone to move to a competitor. Examples of related business questions are \"How does the primary channel (e.g., ATM, visit branch, internet) a bank customer uses affect whether they stay or go?\" or \"Will lower ATM fees significantly reduce the number of high-value customers who leave?\"\n",
    "\n",
    "> ##### Examples:\n",
    "> - Will lower ATM fees significantly reduce the number of high-value customers who leave?\n",
    "> - Does the channel used affect whether customers stay or go?\n",
    "\n",
    "#### 4. ML objectives\n",
    "\n",
    "A list of business objectives which describes the intended outputs of the project that enables the achievement of the business objectives.\n",
    "\n",
    "> ##### Examples:\n",
    "> Predict how many widgets a customer will buy, given their purchases over the past three years, demographic information (age, salary, city, etc.), and the price of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## <span style=\"color:green\">**Background**</span>\n",
    "The music label collaborates with Spotify to gain access to extensive track data. The label aims to identify and promote promising singers by predicting the popularity of songs based on various track features. This dataset includes information on track streams, artist followers, genres, album details, and numerous audio features estimated by Spotify.\n",
    "\n",
    "## <span style=\"color:green\">**Business Problem**</span>\n",
    "The business problem is that the music label wants to efficiently allocate resources to promote the most promising singers. By predicting the popularity of songs, the label can focus on tracks with higher potential for success. The dataset includes various features such as track streams, artist followers, genres, album details, and multiple audio characteristics.\n",
    "\n",
    "## <span style=\"color:green\">**Business Objectives**</span>\n",
    "- Identify tracks with high potential popularity to prioritize marketing efforts.\n",
    "- Determine the impact of specific features on track popularity.\n",
    "\n",
    "## <span style=\"color:green\">**ML Objectives**</span>\n",
    "- Develop a predictive model to estimate the popularity of tracks based on their features.\n",
    "- Provide feature importance analysis to identify key drivers of track popularity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "-------------\n",
    "\n",
    "### tasks\n",
    "- Describe the success criteria of the ML project on three different levels: the business success criteria, the ML success criteria and the economic success criteria.\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. Business success criteria\n",
    "A list of criteria  from a business point of view. For example, if an ML application is planned for a quality check in production and is supposed to outperform the current manual failure rate of 3%, the business success criterion could be derived as e.g. \"failure rate less than 3%\"\n",
    "\n",
    "> ##### Example:\n",
    "> - Increase customer satisfaction ratings by 8% within the next quarter.\n",
    "> - Reduce operational costs by 12% within the next year.\n",
    "\n",
    "\n",
    "#### 2. ML success criteria\n",
    "A list of criteria for a successful outcome to the project in technical terms, for example a certain level of predictive accuracy or a propensity to purchase profile with a given degree of \"lift.\" As with business success criteria, it may be necessary to describe these in subjective terms, in which case the person or persons making the subjective judgment should be identified.\n",
    "\n",
    "> ##### Example:\n",
    "> - The model aims to achieve a recall rate of 95% for identifying customers who are likely to churn.\n",
    "> - The model aims to achieve a precision rate of 90% for identifying high-value customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## <span style=\"color:green\">**Business Success Criteria for Music Label Project:**</span>\n",
    "\n",
    "- Increase the number of top-charting tracks\n",
    "- Improve the hit rate of promoted tracks (tracks that achieve a popularity score above a defined threshold)\n",
    "- Enhance the efficiency of marketing resource allocation, reducing the cost per successful track promotion\n",
    "- Increase the average number of streams per promoted track\n",
    "\n",
    "\n",
    "## <span style=\"color:green\">**ML Success Criteria for Music Label Project:**</span>\n",
    "\n",
    "- Achieve an R-squared value of at least 0.85 for the popularity prediction model, indicating a high level of variance explained by the model.\n",
    "- Ensure the model generalizes well to new data by achieving a cross-validated R-squared value within 5% of the training R-squared value.\n",
    "- Identify and rank the top five most significant features contributing to track popularity with high consistency across different model iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data collection**\n",
    "\n",
    "### tasks\n",
    "- Specify the data sources\n",
    "- Collect the data\n",
    "- Version control on the data\n",
    "\n",
    "#### <span style=\"color:green\">1. Data collection report</span>\n",
    "\n",
    "- **Data Source:** The data was collected from the [zendo](https://zenodo.org/) site. Fostenko Oleg from MSU sampled this data using Spotify API.\n",
    "- **Data Type:** The format or structure of the data, such as numerical, categorical, or text. Example: The data includes many features. Numerical artist_followers, column genres represents list of categorical features, album_release_date (self describing name) and so on.\n",
    "- **Data Size:** The dataset contains 899702 customer records, with 33 features each. Some values missed.\n",
    "- **Data Collection Method:** The data was downloaded from a website manually. After it was split on five parts.\n",
    "\n",
    "\n",
    "#### <span style=\"color:green\">2. Data version control report</span>\n",
    "\n",
    "- **Data Version:** The current data version is v1.0, which was updated on June 18, 2024. Rest four versions stored with versions name like v2.0, ..., v5.0.\n",
    "- **Data Change Log:** A record of changes made to the data. Example: \"The data change log shows that the customer demographics feature was updated on February 20, 2023, to include new categories.\"\n",
    "- **Data Backup:** A copy of data stored remotely on Google disk. \n",
    "- **Data Archiving:** Till now we store all data with archiving on local machine.\n",
    "- **Data Access Control:** Data is stored privately on Google disk. Each team member have equal access to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality verification\n",
    "\n",
    "### tasks\n",
    "- Describe data\n",
    "- Define data requirements\n",
    "- Explore the data\n",
    "- Verify the data quality\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. Data description\n",
    "A section to describe the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Add a table of description of the data features.\n",
    "\n",
    "> ##### Example\n",
    "> - The data acquired for this project includes a dataset of 1000 records with 20 fields each. The fields include customer demographics, purchase history, and product preferences. The data is in a CSV format and is stored in a local database.\n",
    "> - Add a table of description of the data features.\n",
    "\n",
    "#### 2. Data exploration\n",
    "A section to present results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. If appropriate you could include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets.\n",
    "\n",
    "> ##### Example\n",
    "> - During data exploration, several interesting patterns and correlations were discovered. For example, there is a strong correlation between the age of customers and their purchase frequency. Additionally, customers who have purchased from the company in the past are more likely to make repeat purchases. These findings suggest that the data is representative of the target audience and that the company's marketing strategies are effective.\n",
    "> - Add charts and figures to present the findings.\n",
    "\n",
    "#### 2. Data requirements\n",
    "A section to describe the data requirements. The requirements can be defined either on the meta-level or directly in the data, and should state the expected conditions of the data, i.e., whether a certain sample is plausible. The requirements can be, e.g., the expected feature values (a range for continuous features or a list for discrete features), the format of the data and the maximum number of missing values. The bounds of the requirements has to be defined carefully to include all possible real world values but discard non-plausible data. Data that does not satisfy the expected conditions could be treated as anomalies and need to be evaluated manually or excluded automatically. To mitigate the risk of anchoring bias in this first phase discussing the requirements with a domain expert is advised. Documentation of the data requirements could be expressed in the form of a schema with strict data types and conditions.\n",
    "\n",
    "> ##### Example\n",
    "> - The data requirements for this project are defined as follows:\n",
    "> > - Customer Demographics: Age should be within the range of 18 to 100 years, and gender should be either male or female.\n",
    "> > - Purchase History: The number of purchases should be greater than or equal to 0, and the total amount spent should be greater than or equal to $0.\n",
    "> > - Product Preferences: The product preferences should be represented as a list of product IDs, and each product ID should be unique and within the range of 1 to 1000.\n",
    "> > - Here we define expectations to satisfy the data requirements.\n",
    "\n",
    "#### 3. Data quality verification report\n",
    "A section to verify and report the quality of the data. Examine the quality of the data, addressing questions such as:\n",
    "- Is the data complete (does it cover all the cases required)?\n",
    "- Is it correct, or does it contain errors and, if there are errors, how common are they?\n",
    "- Are there missing values in the data? If so, how are they represented, where do they occur, and how common are they?\n",
    "\n",
    "> ##### Example\n",
    "> - Completeness: The data is complete in the sense that it covers all the required cases. All customers have demographic information, and all purchases are recorded.\n",
    "> - Correctness: The data appears to be correct, with no obvious errors. However, a manual review of the data is recommended to ensure that there are no errors.\n",
    "> - Missing Values: There are no missing values in the data.\n",
    "> - Overall, the data quality is high, and the data is suitable for analysis and modeling. However, a manual review of the data is recommended to ensure that there are no errors or anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about the features present in the dataset can be viewed in report generated below.\n",
    "Exception: genres and available_markets, they will be evaluated in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "report_path = \"../reports/profile_report.html\"\n",
    "df = pd.read_csv(\"../data/tracks.csv\")\n",
    "# album_release_date originaly is string with mixed format (some tracks have only year, some have full date)\n",
    "# so we need to parse it correctly\n",
    "df[\"album_release_date\"] = pd.to_datetime(df[\"album_release_date\"], format=\"mixed\", yearfirst=True, errors=\"coerce\")\n",
    "df[\"added_at\"] = pd.to_datetime(df[\"added_at\"], yearfirst=True, errors=\"coerce\")\n",
    "\n",
    "if os.path.exists(report_path):\n",
    "    pass\n",
    "else:\n",
    "    # Run profiling and generate new report\n",
    "    profile_report = ProfileReport(df)\n",
    "    profile_report.to_file(report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some insights from the report\n",
    "\n",
    "#### track_id\n",
    "- Holds the id of the track\n",
    "- Gives no valuable information for our task, thus can be <span style=\"color:red\">ignored</span>.\n",
    "\n",
    "#### streams\n",
    "- number of times the track has been listened to\n",
    "- Important feature at first glance, but nevertheless can't be used due to <span style=\"color:red\">high missing rate</span>\n",
    "\n",
    "#### track_artists\n",
    "- name of the track's author\n",
    "- Has a <span style=\"color:orange\">high missing rate</span>, but maybe it can be solved using information about album\n",
    "\n",
    "#### artist_popularity\n",
    "- popularity of the track's author estimated by Spotify\n",
    "- has a <span style=\"color:orange\">second peak at 0</span> so we will have to cope with that\n",
    "\n",
    "#### explicit\n",
    "- whether the lyrics contain obscene words\n",
    "- Has a <span style=\"color:orange\">high imbalance</span>, which is expected. Maybe we could make it as an additional subgenre, thus making this feature a part of genre distribution\n",
    "\n",
    "#### chart\n",
    "- chart the track is in (if any)\n",
    "- Has a <span style=\"color:orange\">high missing rate</span>, which is expected (not all songs get into topN charts).\n",
    "\n",
    "#### key\n",
    "- track's tonality estimated by Spotify\n",
    "- some values are <span style=\"color:orange\">underrepresented</span>\n",
    "\n",
    "#### added_at\n",
    "- moment in time when the track was uploaded\n",
    "- Has a <span style=\"color:orange\">mild missing rate</span>. There are 2 possible solutions: we either try to restore the time using album_release_date and some other information like release time of other trecks from the album, or we will just use the <span style=\"color:green\">album_release_date</span> instead\n",
    "\n",
    "#### popularity\n",
    "- track's popularity, the feature we try to predict\n",
    "- Has a <span style=\"color:orange\">huge imbalance on 0</span>. \n",
    "\n",
    "#### track_album_album\n",
    "- type of the album the track is a part of\n",
    "- Has a <span style=\"color:red\">high missing rate</span>\n",
    "\n",
    "#### duration_ms\n",
    "- length of the track in milliseconds\n",
    "- Has a <span style=\"color:red\">high missing rate</span>.\n",
    "\n",
    "#### track_track_number\n",
    "- track's disc number according to Spotify (the number of the track in the album it belongs to)\n",
    "- Has a <span style=\"color:red\">high missing rate</span>. No explicit influence on popularity\n",
    "\n",
    "#### rank\n",
    "- position in the chart (if the track is a part of a chart)\n",
    "- dependent of song being in chart which is already a feature with a <span style=\"color:red\">high missing rate</span>\n",
    "\n",
    "#### time_signature\n",
    "- time signature of the track\n",
    "- Has a <span style=\"color:red\">high impbalance on signature 4</span>. \n",
    "\n",
    "#### region\n",
    "- region of the chart (if track is a part of any chart)\n",
    "- Same problem as with the rank. May be ignored due to <span style=\"color:red\">high correlation and missign rate</span>\n",
    "\n",
    "#### trend\n",
    "- change in track's position within the chart (if track is a part of a chart)\n",
    "- Same problem as with the rank. May be ignored due to <span style=\"color:red\">high correlation and missign rate</span>\n",
    "\n",
    "#### instrumentalness\n",
    "- instrumentalness of the track estimated by Spotify\n",
    "- Has a <span style=\"color:orange\">high impbalance on 0</span>. Using assumption that instrumental music is less popular we can say that this is expected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored 17.78% of missing artists\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Let's try to restore the artist name\n",
    "missing_tracks = df[df[\"track_artists\"].isnull()][[\"track_artists\", \"album_name\", \"name\"]]\n",
    "# For each album name, find the list of associated artists.\n",
    "album_artists = df.groupby(\"album_name\")[\"track_artists\"].apply(lambda x: x.dropna().unique())\n",
    "# Drop albums with no associated artists\n",
    "album_artists = album_artists[album_artists.apply(len) > 0]\n",
    "# Fill in missing artists\n",
    "missing_tracks[\"track_artists\"] = missing_tracks[\"album_name\"].map(album_artists)\n",
    "# Check how many artists restored\n",
    "print(f\"Restored {np.sum(missing_tracks['track_artists'].notnull()) / len(missing_tracks) * 100:.2f}% of missing artists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after restoring some artists, feature still has a high missing rate, so we can't use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go to some interesting information that we can use\n",
    "\n",
    "#### genres\n",
    "- track genres\n",
    "- report generator missclassified this feature as a text than in fact it is a list of categorical data\n",
    "- some tracks don't have any genres specified, but this is not a problem as some tracks can have no genre at all\n",
    "- some genres are just combinations of more simple ones, so we will have to deal with that\n",
    "- we will just one hot encode this feature\n",
    "\n",
    "#### available_markets\n",
    "- generated report suggests that these features have only a little variation and appear in entries mostly in 2 forms:\n",
    "    - empty list\n",
    "    - list containing nearly all available categories\n",
    "- Thus, we need to explore further its influence on popularity\n",
    "\n",
    "#### Handling the missing values\n",
    "- In this dataset we have 2 cases:\n",
    "    - feature has around 0.1% missing values that can be solved without any troubles with mean or most frequent values\n",
    "    - feature has around 90% and more missing values, thus making it impossible to fill in any values\n",
    "- In both cases above solutions are very simple: either drop the feature or fill in without any troubles\n",
    "- Exception is added_at feature, but even here it's just a matter of whether we need precise time with <span style=\"color:red\">high missing rate</span> or release date will be enough\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genres restored\n",
      "available_markets restored\n",
      "Dataset restored with 5424 columns\n",
      "Dataset reduced to 5412 columns\n",
      "Chart restored\n",
      "Missing values imputed\n"
     ]
    }
   ],
   "source": [
    "# Let's try to find some patterns in the data\n",
    "# First, let's restore the categorical data genres, available_markets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "pattern_df = df.copy()\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_categories = pd.DataFrame(mlb.fit_transform(pattern_df[\"genres\"].apply(eval)), columns=mlb.classes_, index=pattern_df.index)\n",
    "print(\"genres restored\")\n",
    "available_markets_categories = pd.DataFrame(mlb.fit_transform(pattern_df[\"available_markets\"].apply(eval)), columns=mlb.classes_, index=pattern_df.index)\n",
    "print(\"available_markets restored\")\n",
    "pattern_df.drop(columns=[\"genres\", \"available_markets\"], inplace=True)\n",
    "pattern_df = pd.concat([pattern_df, genres_categories, available_markets_categories], axis=1)\n",
    "print(f\"Dataset restored with {pattern_df.shape[1]} columns\")\n",
    "\n",
    "# Now we will drop some columns\n",
    "pattern_df.drop(columns=[\n",
    "    \"track_id\", # unique identifier\n",
    "    \"streams\", # Too many missing values\n",
    "    \"track_artists\", # Too many missing values\n",
    "    \"added_at\", # Too many missing values and can be replaced by album_release_date\n",
    "    \"track_album_album\", # Too many missing values\n",
    "    \"duration_ms\", # Too many missing values\n",
    "    \"track_track_number\", # Too many missing values\n",
    "    \"rank\", # Too many missing values and dependent of chart\n",
    "    \"album_name\", # This is text data and we do not do data transformation in this phase\n",
    "    \"region\", # Too many missing values\n",
    "    \"trend\", # Too many missing values and dependent of chart\n",
    "    \"name\", # This is text data and we do not do data transformation in this phase\n",
    "], inplace=True)\n",
    "print(f\"Dataset reduced to {pattern_df.shape[1]} columns\")\n",
    "\n",
    "# Replace nan chart with 0, top200 with 1 and top50 with 2\n",
    "pattern_df[\"chart\"] = pattern_df[\"chart\"].map({\"top200\": 1, \"top50\": 2})\n",
    "pattern_df[\"chart\"] = pattern_df[\"chart\"].fillna(0)\n",
    "print(\"Chart restored\")\n",
    "\n",
    "# Convert album_release_date\n",
    "pattern_df[\"album_release_date\"] = pattern_df[\"album_release_date\"].astype(\"int64\")\n",
    "\n",
    "# Now we will impute the missing values\n",
    "# Since the number of mising values is small, it is safe to use the median value\n",
    "pattern_df.fillna(pattern_df.median(), inplace=True)\n",
    "print(\"Missing values imputed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to find some patterns\n",
    "\n",
    "continuous_columns = [\n",
    "    \"artist_followers\",\n",
    "    \"album_total_tracks\",\n",
    "    \"artist_popularity\",\n",
    "    \"tempo\",\n",
    "    \"album_release_date\",\n",
    "    \"energy\",\n",
    "    \"popularity\",\n",
    "    \"speechiness\",\n",
    "    \"danceability\",\n",
    "    \"valence\",\n",
    "    \"liveness\",\n",
    "    \"instrumentalness\",\n",
    "    \"loudness\",\n",
    "]\n",
    "\n",
    "# Make pairplot with popularity column\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.pairplot(pattern_df.sample(1000), vars=continuous_columns, hue=\"popularity\")\n",
    "plt.savefig(\"../reports/pairplot.png\")\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the generated pairplot we can see that there is a clear dependency between song popularity and the artist's popularity.\n",
    "We can also see, not so obvious, but still considerable relation between loudness, danceability, and popularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZCQA0YfF1Yq"
   },
   "source": [
    "## Project feasibility \n",
    "-------------\n",
    "This task involves more detailed fact-finding about all of the resources,constraints, assumptions and other factors that should be considered in determining the data analysis goal and project plan. In the previous task, your objective is to quickly get to the crux of the situation. Here, you want to flesh out the details.\n",
    "\n",
    "### tasks\n",
    "- Assess the project feasibility\n",
    "- Create POC (Proof-of-concept) model\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. inventory of resources\n",
    "\n",
    "List the resources available to the project, including: personnel (business experts, data experts, technical support, machine learning personnel), data (fixed extracts, access to live warehoused or operational data), computing resources (hardware platforms) and software (machine learning tools, other relevant software).\n",
    "\n",
    "#### 2. Requirements, assumptions and constraints\n",
    "\n",
    "List all requirements of the project including schedule of completion, comprehensibility and quality of results and security as well as legal issues.As part of this output, make sure that you are allowed to use the data. List the assumptions made by the project. \n",
    "\n",
    "These may be assumptions about the data that can be checked during machine learning, but may also include non-checkable assumptions about the business upon which the project rests. It is particularly important to list the latter if they form conditions on the validity of the results.\n",
    "\n",
    "List the constraints on the project. These may be constraints on the availability of resources, but may also include technological constraints such as the size of data that it is practical to use for modeling.\n",
    "\n",
    "#### 3. Risks and contingencies \n",
    "\n",
    "List the risks or events that might occur to delay the project or cause it to fail. List the corresponding contingency plans; what action will be taken if the risks happen.\n",
    "\n",
    "\n",
    "#### 4. Costs and benefits\n",
    "\n",
    "Construct a cost-benefit analysis for the project, which compares the costs of the project with the potential benefit to the business if it is successful. The comparison should be as specific as possible.\n",
    "\n",
    "![](https://i.imgur.com/XU2lghc.png)\n",
    "\n",
    "#### 5. Feasibility report\n",
    "\n",
    "Build a POC ML model and explain as a team whether it is feasible to do this ML project or not. If not, then you need to find another business problem. They key factors here are related to data availability, quality, costs and nature of business problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this task we will avoid any data preprocessing\n",
    "# We will train the HistGradientBoostingRegressor model on the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# We will use only numberical and categorical features for now\n",
    "POC_data = df.select_dtypes(include=[\"number\", \"bool\"])\n",
    "print(f\"Without text data: {POC_data.shape}\")\n",
    "\n",
    "# We can also add some categorical features\n",
    "genres_info = df[\"genres\"].apply(eval)\n",
    "genres_counts = genres_info.explode().value_counts()\n",
    "# Binarize only genres with at least 200 occurrences\n",
    "genres_mlb = MultiLabelBinarizer(classes=genres_counts[genres_counts >= 200].index)\n",
    "\n",
    "available_markets_info = df[\"available_markets\"].apply(eval)\n",
    "available_markets_mlb = MultiLabelBinarizer()\n",
    "genres_info = pd.DataFrame(genres_mlb.fit_transform(genres_info), columns=genres_mlb.classes_, index=genres_info.index)\n",
    "available_markets_info = pd.DataFrame(available_markets_mlb.fit_transform(available_markets_info), columns=available_markets_mlb.classes_, index=available_markets_info.index)\n",
    "POC_data = pd.concat([POC_data, genres_info, available_markets_info], axis=1)\n",
    "print(f\"With categorical features: {POC_data.shape}\")\n",
    "\n",
    "# Drop the features with high missing rate\n",
    "POC_data = POC_data.dropna(axis=1, thresh=len(POC_data) * 0.8)\n",
    "# Drop rows with missing values\n",
    "POC_data = POC_data.dropna()\n",
    "print(f\"After dropping missing values: {POC_data.shape}\")\n",
    "\n",
    "X = POC_data.drop(columns=[\"popularity\"])\n",
    "y = POC_data[\"popularity\"]\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "\n",
    "# Create the grid search model\n",
    "grid_search = GridSearchCV(estimator=HistGradientBoostingRegressor(), param_grid=param_grid, cv=5, n_jobs=-1, scoring='r2', verbose=3)\n",
    "\n",
    "# Fit the grid search model to the training data\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the model using SHAP values\n",
    "import shap\n",
    "\n",
    "# Print r2 score first\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n",
    "# Create the explainer\n",
    "explainer = shap.Explainer(grid_search.best_estimator_)\n",
    "# Calculate the SHAP values\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot the SHAP values\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is just a POC model we didn't consider time data and text data for now as this will be handled at data preprocessing stages.\n",
    "We trained a simple model on a numerical and categorical data only and acheaved 70% r2 score which we can consider as a success for the POC.\n",
    "Some insights from the results:\n",
    "- adding genres and available data along side with just numerical data significantly boosted POC model performance\n",
    "- without categorical data model showed only 31% (not shown in this notebook)\n",
    "\n",
    "Some conclusions we can make:\n",
    "- Performance boost acquired from the categorical data suggests that finding a way of using the data left out of POC model sigh will help us boost the model performance even more\n",
    "- for POC modelling we dropped some entries from the dataset which is not a good practice, but in our case, as was described before, we have 2 cases: either feature has a major missing rate, making it impossible to use this feature, or feature has only a few entries missing, making filling in the values insignificant for POC modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## produce project plan\n",
    "----------------\n",
    "\n",
    "### task\n",
    "\n",
    "Describe the intended plan for achieving the machine learning goals and thereby achieving the business goals. The plan should specify the anticipated set of steps to be performed during the rest of the project including an initial selection of tools and techniques.\n",
    "\n",
    "### output\n",
    "\n",
    "#### 1. Project plan\n",
    "List the stages to be executed in the project, together with their duration, resources required, inputs, outputs, and dependencies. Where possible, try and make explicit the large-scale iterations in the machine learning process, for example, repetitions of the modeling and evaluation phases. As part of the project plan, it is also important to analyze dependencies between time schedule and risks. Mark results of these analyses explicitly in the project plan, ideally with actions and recommendations if the risks are manifested. Decide at this point which evaluation strategy will be used in the evaluation phase. Your project plan will be a dynamic document. At the end of each phase you’ll review progress and achievements and update the project plan accordingly. Specific review points for these updates should be part of the project plan.\n",
    "\n",
    "#### 2. ML project Canvas\n",
    "At the end of the first phase, you should create a canvas for the project as a summary of this phase. \n",
    "\n",
    "\n",
    "> ##### Example\n",
    "> Follow the link: https://github.com/louisdorard/machine-learning-canvas/blob/master/churn.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP0tcb9vN+g/E9NCP5Xbry9",
   "collapsed_sections": [],
   "name": "business-understanding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
